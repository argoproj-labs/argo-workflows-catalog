apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  annotations:
    workflows.argoproj.io/description: >-
      this workflow template contains a template to make simple test-container executed via argo.
    workflows.argoproj.io/maintainer: '@sumitnagal'
    workflows.argoproj.io/tags: distro
    workflows.argoproj.io/version: '>= 2.9.0'
  name: distro
  generateName: perf-infra-
spec:
  serviceAccountName: argowf-svcacc
  entrypoint: perf-infra
  poddisruptionbudget:
    minavailable: 100%
  # must complete in 1m
  activeDeadlineSeconds: 86400
  # keep workflows for 3m
  ttlStrategy:
    secondsAfterCompletion: 3600
  # delete all pods as soon as they complete
  podGC:
    strategy: OnPodCompletion
  arguments:
    parameters:
      - name: limit
        value: 1
      - name: peakTPS
        value: 1
      - name: rampupTime
        value: 1
      - name: steadyStateTime
        value: 1
      - name: gitrepo
        value: "NA"
      - name: buildURL
        value: "NA"
      - name: envName
        value: "prf"
      - name: buildnum
        value: 1
      - name: baseurl
        value: "https://XXX.amazonaws.com"
      - name: uniqueName
        value: "{{workflow.creationTimestamp}}"
      - name: dataFlag
        value: "false"
      - name: appTestImg
        value: "distroproj/gatling:latest"
      - name: awscliGatmergeImg
        value: "distroproj/gatling-merge:latest"
      - name: pfiNamespace
        value: "infra-ns"
      - name: query
        value: "/health/full"
      - name: simulationClass
        value: "Echo.EchoSimulation"
      - name: s3BucketName
        value: perf-results-XXXX
  templates:
    - name: perf-infra
      steps:
        - - name: pdbcreate
            template: pdbcreate 
        - - name: run-test
            template: run-test
            withSequence:
              count: "{{workflow.parameters.limit}}"
        - - name: list-test
            template: list
        #  Un comment if you want to fetch reports via Jenkins or other CI systesm
        # - - name: s3-access
        #     template: s3access

    - name: pdbcreate
      container:
        image: alpine:latest
        command: [sh, -c]
        args: [sleep 10]
    
    - name: run-test
      metadata:
        # If you are using role base access for your namespace configure this
        # annotations:
        #   iam.amazonaws.com/role: "k8s-{{workflow.parameters.pfiNamespace}}"
      container:
        image: "{{workflow.parameters.appTestImg}}"
        imagePullPolicy: Always
        command: ["sh","-c"]
        args: [
          "cd /gatling && mvn gatling:test -Dgatling.simulationClass={{workflow.parameters.simulationClass}} -Durl={{workflow.parameters.baseurl}} -Dquery={{workflow.parameters.query}} -DpeakTPS={{workflow.parameters.peakTPS}} -DrampupTime={{workflow.parameters.rampupTime}} -DsteadyStateTime={{workflow.parameters.steadyStateTime}} && cp -r /gatling/gatling_results /tmp && mkdir /tmp/simulations && cp $(find . -name 'simulation.log' | tail -1) /tmp/simulations/simulation_$(date +'%s')_$(cat /dev/urandom | env LC_CTYPE=C tr -cd 'a-f0-9' | head -c 6).log && ls /tmp"
        ]
        resources:                # limit the resources
          requests:
            memory: 1Gi
            cpu: "1"
          limits:
            memory: 2Gi
            cpu: "1"
        # volumeMounts:
        # - name: finalresults
        #   mountPath: /tmp
      outputs:
        artifacts:
          - name: simulation
            path: /tmp/simulations
            # It is possible to disable tar.gz archiving by setting the archive strategy to 'none'
            # Disabling archiving has the following limitations on S3: symboloic links will not be
            # uploaded, as S3 does not support the concept/file mode of symlinks.
            # archive:
            #   none: {}

            s3:
              # Use the corresponding endpoint depending on your S3 provider:
              #   AWS: s3.amazonaws.com
              #   GCS: storage.googleapis.com
              #   Minio: my-minio-endpoint.default:9000
              endpoint: s3.amazonaws.com
              bucket: "{{workflow.parameters.s3BucketName}}"
              region: us-west-2
              key: results/{{workflow.namespace}}/{{workflow.parameters.uniqueName}}/simulations/{{pod.name}}.tgz
              accessKeySecret:
                name: my-s3-credentials
                key: accessKey
              secretKeySecret:
                name: my-s3-credentials
                key: secretKey

    - name: list
      container:
        image: "{{workflow.parameters.awscliGatmergeImg}}"
        command: [sh, -c]
        args: ["
          cd /perf-in &&
          ls -R &&
          for a in `ls -1 *.tgz`; do
              gzip -dc $a | tar xf -;
          done &&
          echo '=======' &&
          ls -R /perf-in &&
          cd /opt/gatling/bin &&
          ./gatling.sh -ro /perf-in/simulations &&
          mkdir /perf-out &&
          cp -rf /perf-in/simulations /perf-out/simulations &&
          rm $(find /perf-out/ -name '*.log') &&
          ls -R /perf-out
        "]
        resources:        # limit the resources
          requests:
            memory: 1Gi
            cpu: "1"
          limits:
            memory: 2Gi
            cpu: "1"
      metadata:
        # annotations:
        #   iam.amazonaws.com/role: "k8s-{{workflow.parameters.pfiNamespace}}"
      inputs:
        artifacts:
          - name: inputsimulations
            path: /perf-in
            s3:
              # Use the corresponding endpoint depending on your S3 provider:
              #   AWS: s3.amazonaws.com
              #   GCS: storage.googleapis.com
              #   Minio: my-minio-endpoint.default:9000
              endpoint: s3.amazonaws.com
              bucket: "{{workflow.parameters.s3BucketName}}"
              region: us-west-2
              key: results/{{workflow.namespace}}/{{workflow.parameters.uniqueName}}/simulations/
              accessKeySecret:
                name: my-s3-credentials
                key: accessKey
              secretKeySecret:
                name: my-s3-credentials
                key: secretKey

      outputs:
        artifacts:
          - name: finalResult
            path: /perf-out
            # It is possible to disable tar.gz archiving by setting the archive strategy to 'none'
            # Disabling archiving has the following limitations on S3: symboloic links will not be
            # uploaded, as S3 does not support the concept/file mode of symlinks.
            # this will work on your local machine as well as any of ci system as result will be uploaded to S3
            archive:
              none: {}

            s3:
              # Use the corresponding endpoint depending on your S3 provider:
              #   AWS: s3.amazonaws.com
              #   GCS: storage.googleapis.com
              #   Minio: my-minio-endpoint.default:9000
              endpoint: s3.amazonaws.com
              bucket: "{{workflow.parameters.s3BucketName}}"
              region: us-west-2
              key: results/{{workflow.namespace}}/{{workflow.parameters.uniqueName}}/finalResult/
              accessKeySecret:
                name: my-s3-credentials
                key: accessKey
              secretKeySecret:
                name: my-s3-credentials
                key: secretKey

    - name: s3access
      metadata:
        annotations:
          iam.amazonaws.com/role: "k8s-{{workflow.parameters.pfiNamespace}}"
      container:
        image:  "{{workflow.parameters.awscliGatmergeImg}}"
        command: [sh, -c]
        args: ["sleep 20 && aws s3 ls s3://{{workflow.parameters.s3BucketName}}/results/ && aws s3 cp --recursive --acl authenticated-read 's3://{{workflow.parameters.s3BucketName}}/results/{{workflow.namespace}}/{{workflow.parameters.uniqueName}}' 's3://{{workflow.parameters.s3BucketName}}/results/{{workflow.namespace}}/{{workflow.parameters.uniqueName}}'"]